---
title: "Globalization: Differences between Brexit and Trump Election"
author: "Jingjing Luo"
date: "2022-08-02"
output: html_document
---
<br>


##### Abstract

This report looks into the differences between tweets made by British Parliament members who support Brexit and tweets made by Donald Trump during the 2016 election period. These two topics are both anti-globalization events happening at similar times. I wanted to see whether different anti-globalization events correlate with different types of sentiments. First, I built a two models for both datasets that classifies **tweets** into *support* for the specific anti-globalization cause vs *against* the anti-globalization cause. Then, I used the **text** to calculate and compare the sentiments of the two groups. I found there to be a difference in the type of language used between the two groups. In conclusion, there are definitely some sentiment differences between people who support anti-globalization.




<br>

##### Background
I found the Election Dataset from [Kaggle](https://www.kaggle.com/datasets/benhamner/clinton-trump-tweets) and it consisted of ~3000 tweets from each of the 2016 US Presidential Election candidates, Donald Trump and Hillary Clinton. I found the Brexit Dataset from [Kaggle](https://www.kaggle.com/datasets/chadjinik/labelledbrexittweets) as well, and it consists of 60,941 tweets related to Brexit from 449 UK members of Parliament. After my first analysis, I filtered out both datasets to be left with only tweets from Trump or tweets from Parliament members who support Brexit. 





<br>

##### Research Question and Hypothesis

What are the sentiment differences between tweets made by Members of Parliament who support Brexit vs those made by Trump? 

* Are there different sentiments, tone, and levels of passion depending on which cause people support?

Hypotheses:

* Part 1
  + Null Hypothesis: The stance British Parliament members have on Brexit does not have any correlation with the sentiment of the tweet.
  + Alternate Hypothesis: The sentiment of the tweet has correlation with the stance British Parliament members have on Brexit. 
* Part 2
  + Null Hypothesis: The Trump Dataset and Brexit Dataset have the same sentiments
  + Alternate Hypothesis: The Brexit Dataset different sentiments with the Trump Dataset.


<br>

#### Figure 1

``` {r, warning=F,echo=F}
#### Load Data ####
election_data <- read.csv("JingjingsProject/data/tweets.csv")


brexit_data <- read.csv("JingjingsProject/data/brexit.csv")

#### Graph the Results ####
library(ggplot2)

accuracy_set <- data.frame(sets = c("Sentiment Function", "Starspace on Brexit", "Starspace on Election", "Sentiment Function", "Starspace on Brexit", "Starspace on Election"), accuracy = c(0.6861832, 0.9813546, 0.9857232, 0.5, 1, 1), calculation = c("accuracy", "accuracy", "accuracy", "f1", "f1", "f1"))

ggplot(accuracy_set, aes(x = sets, y = accuracy, color = calculation)) +
  geom_point(stat="identity", position = "dodge") +
  labs(x = "Models and Datasets", y = "score", title = "Accuracy and F1 of Different Models for the Brexit and Trump Datasets") +
  theme(text = element_text(family = "Times New Roman"), axis.text.x = element_text(angle = 20))

```

This plot shows the accuracies of the two classification models on the two datasets. First, I used a simple sentiment function to split the Brexit `text` into "positive" or "negative". Then, I used the sentiment data as my variable in a general boosting machine to try and classify the Brexit stance as either `Leave` or `Stay`. This resulted in the model classifying everything as `Stay` because there was little correlation between the sentiment and the stance. Therefore, I tried out another model for classification. This Starspace model takes the `text` and splits it into the `Leave` or `Stay` categories through document embeddings. This model proved to be very accurate, so I took the model and tested it on the Election Dataset. The accuracy and f1 score were very similar for the two datasets, with the Election dataset having slightly higher accuracy.

<br>

#### Figure 2
``` {r, echo = F, warning = F, fig.show="hold",out.width="50%", message = F}
#### Graph 

#### Get libraries ####
library(tidytext)
library(textdata)
library(dplyr)
library(stringr)

#### Clean Data ####
brexit_data3 <- brexit_data

brexit_data3$plain <- strsplit(brexit_data3$text, "\\W")
brexit_data3$plain <- sapply(brexit_data3$plain, FUN=function(x) paste(x, collapse = " "))
brexit_data3$plain <- tolower(brexit_data3$plain)

election_data_samp <- election_data

election_data_samp$plain <- strsplit(election_data_samp$text, "\\W")
election_data_samp$plain <- sapply(election_data_samp$plain, FUN=function(x) paste(x, collapse = " "))
election_data_samp$plain <- tolower(election_data_samp$plain)

election_data_samp <- election_data_samp %>%
  dplyr::rename(screen_name = handle) %>%
  filter(screen_name == "realDonaldTrump") %>%
  select(screen_name, plain)

# after cleaning election data

brexit_data_samp <- brexit_data3 %>%
  filter(label == "Leave") %>%
  select(screen_name, plain)

## random sample of 5000 rows
brexit_3k <- slice_sample(brexit_data_samp, n = 3000)
election_3k <- slice_sample(election_data_samp, n = 3000)

## combine into 1 dataset
word_data <- rbind(brexit_3k, election_3k)

# get rid of urls
word_data$plain <- str_replace_all(string = word_data$plain,
                                            pattern = "https.+",
                                            replacement = "")

#### get sentiments ####

#get_sentiments("nrc")


#### Analysis ####
tidy_books <- word_data %>%
  unnest_tokens(word, plain)

tidy_books <-tidy_books %>%
  mutate(stance = ifelse(screen_name == "realDonaldTrump", "trump", "brexit"))

## bar plot with x at diff emotions with different colors showing the count of each emotion
# find 1 emotion per screen name no named trump
no_trump_sentiments <- tidy_books %>%
  filter(screen_name != "realDonaldTrump") %>%
  inner_join(get_sentiments("nrc"))

sentiment_concise <- no_trump_sentiments %>%
  group_by(screen_name, sentiment) %>%
  dplyr::summarize(count = n()) %>%
  group_by(screen_name) %>%
  summarize(sentiment = ifelse(count == max(count), sentiment,"")) %>%
  filter(sentiment != "") %>%
  slice_sample(n=1)


#ggplot(sentiment_concise, aes(x=sentiment, fill = sentiment)) +
  #geom_bar(aes(y = (..count..)/sum(..count..))) +
  #labs(x = "Sentiment", y = "Percent of Total", 
       #title = "Sentiments of Twitter Users who Support Brexit") +
  #theme(text = element_text(family = "Courier"))


#sentiments w/o condensing
ggplot(no_trump_sentiments, aes(x=sentiment, fill = sentiment)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  labs(x = "Sentiment", y = "Percent of Total", 
       title = "Twitter Sentiments of Parliament Members who Support Brexit") +
  theme(text = element_text(family = "Courier"), axis.text.x = element_text(angle = 20))
  

#trump sentiments
trump_sentiments <- tidy_books %>%
  filter(screen_name == "realDonaldTrump") %>%
  inner_join(get_sentiments("nrc"))

ggplot(trump_sentiments, aes(x=sentiment)) +
  geom_bar(aes(y = (..count..)/sum(..count..)), fill = "deeppink4") +
  labs(x = "Sentiment", y = "Percent of Total", 
       title = "Trump Twitter Sentiments") +
  theme(text = element_text(family = "Courier"), axis.text.x = element_text(angle = 20))


```
<br>

This plot compares sentiments Trump expresses in tweets vs sentiments pro-Brexit Parliament members express in tweets. I used the `nrc emotions` data table to classify my text. The previous analysis showed that there was no correlation between sentiment and the two sides of **one** event, but I wanted to see whether there would be different sentiments between **different** anti-globalization events. The conclusion is that Trump's tweets contain more anger sentiments and disgust sentiments while the Brexit tweets contain more negative sentiments, suprise sentiments, and trust sentiments. However, the two plots are overall very similar.


<br>

#### Figure 3.1
```{r, echo=F, message = F, fig.show="hold", out.width="50%"}

# graph a list of top 10 most popular words with anger and disgust (separate) and their counts
#trump

nrc_anger <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")

anger_words_t <- tidy_books %>% 
  filter(screen_name == "realDonaldTrump") %>%
  inner_join(nrc_anger) %>%
  dplyr::count(word, sort = TRUE) %>%
  slice_head(n = 10)


#brexit
anger_words_b <- tidy_books %>% 
  filter(screen_name != "realDonaldTrump") %>%
  inner_join(nrc_anger) %>%
  dplyr::count(word, sort = TRUE) %>%
  slice_head(n = 10)


## plot of trump's top 10 anger words
ggplot(anger_words_t, aes(x = reorder(word,(-n)), y = n)) +
  geom_bar(stat = "summary",
           fun = "mean",
           fill = "indianred") +
  labs(x = "Word", y = "Count", title = "Top 10 most common words Trump uses when angry") +
  theme(text = element_text(family = "Courier"), axis.text.x = element_text(angle = 45))

## plot of brexit's top 10 words
ggplot(anger_words_b, aes(x=reorder(word,(-n)), y=n)) +
  geom_bar(stat = "summary",
           fun = "mean",
           fill = "skyblue4") +
  labs(x = "Word", y = "Count", title = "Top 10 most common anger words Brexit Parliament supporters use") +
  theme(text = element_text(family = "Courier"), axis.text.x = element_text(angle = 45))

```

<br>

These plots show the **Top 10 Most Common Words Expressing Anger** used by Brexit Parliament supporters and Trump. Because the previous analysis of Brexit supporters' sentiments and Trump's sentiments were more similar than expected, I then tried to evaluate whether there are differences within each sentiment. Are the words used to express each sentiment different? The conclusion is that the words used truly are different, but it should be noted that the words `vote, money, campaigning, and politics` are typically not thought of as words expressing anger. They are possible mistakes in the "nrc" data table I used to classify the tweets. These *mistakes* likely affected the previous sentiment analysis to have such similar results between the two datasets. If the *mistakes* are removed, it can be seen that Trump uses more words expressing anger in his tweets. Overall, both sides use different words to express their sentiments.  

<br>

#### Figure 3.2
```{r, echo = F, message = F}
## what words contributed to certain sentiments
nrc_word_counts <- tidy_books %>%
  inner_join(get_sentiments("nrc")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()

nrc_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL) 

```
<br>

This plot shows the top 10 most common words seen in the twitter text for each emotion. In this plot, we can clearly see more and more words that are misinterpreted affect the sentiment. There are words like `donald, vote, money, campaining, politics` that don't belong to their respective sentiment categories. 

##### Discussion
<br>

I can conclude that there is not a strong correlation between the tweet sentimient and the stance Parliament members have in the Brexit Dataset. The data fails to reject the first null hypothesis. However, I can conclude that there are sentiment differences between the **two** datasets. These slight sentiments differences are apparent when 11 different sentiments are represented. Moreover, the two groups of twitter uses have different word choices when expressing similar sentiments. This supports the second alternate hypothesis. 

Because the first null hypothesis was no rejected, I had to find another way to accurately classify the sentiments. The new Starspace model I used worked well with a 98% accuracy. 

This research shows that different people who support anti-globalization can have very different sentiments. This research could be improved if I found more globalization-specific datasets to analyze. In addition, the "nrc" data table I found to calculate the sentiments of each dataset misinterpreted many words, such as "vote" and "politics". This definitely affected the sentiment comparisons between the two datasets and made the plot less accurate. 

I want to continue this research by testing whether or not there is a relationship between someone's social status and their reasons for supporting some anti-globalization causes. In addition, I want to continue to test the sentiments of different supporters using a better data table. Furthermore, I could include more datasets from both Britain and America to see if anti-globalization reasons correlate with people in the same country, same education level, or same age group etc.

This research is very valuable but I think there could be a lot more done. The debate on *how much globalization is too much?* continues, and understanding the perspectives of people from the **globe** would be very valuable to working together to come up with a solution. This is the [GitHub](https://github.com/the-codingschool/custom-ds-projects) link to my code and analysis.
